# -*- coding: utf-8 -*-
"""jax_gemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/jax_gemma.ipynb

# JAX Gemma on Colab TPU

Gemma is a family language models released by Google DeepMind in the paper [Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf). Gemma leverages the same research and technology used to create the Gemini models. It was trained using a dataset consisting of 6-trillion tokens, formed from web documents, code and mathematics. The result is a series of state-of-the-art models at the 2B and 7B scale, all of which are open-sourced and permissively licensed.

JAX is a Python library for high-performance machine-learning research. It uses the XLA (accelerator linear algebra) compiler to fuse sequences of operations together and run them at once. In the context of machine-learning, this allows JAX to execute blocks of modelling code very efficiently on modern hardware accelerators, like GPUs and TPUs, making it appealing for large-scale research and applications. Gemma itself was trained using JAX on TPU v5e cores.

Gemma has support in the ðŸ¤— Transformers library from day-1, in both PyTorch and JAX. In this Google Colab, we'll showcase how to use the Gemma 2B model in JAX, leveraging the power of TPU v2 cores to run batched generation with a throughput of 475 tokens per second. For details on using Gemma in PyTorch on GPU, refer to the corresponding [documentation](https://huggingface.co/google/gemma-2b#usage).

## Connect to a TPU

First, we need to register our Hugging Face Hub token with our Google Colab runtime. Since the Gemma model is gated, our token will be checked when the model is downloaded to ensure we have accepted the terms-of-use.

To register your token, click the key symbol ðŸ”‘ in the left-hand pane of the screen. Name the secret `HF_TOKEN`, and copy a token from your Hugging Face Hub account: https://huggingface.co/settings/tokens. Your token should now be registered, allowing you to access the Gemma weights to this Colab session.

Next, we can connect to a TPU. Google Colab offers TPU v2's on its free tier, which we'll make use of for this notebook. If you have access to later generations of TPU, such as v3, v4 or v5e machines, you can run the same notebook and achieve significantly faster generation speeds.

To connect to a TPU v2, click on the button `Connect TPU` in the top right-hand corner of the screen. Once connected to a TPU, we need to initialise it with the `setup_tpu` method:
"""


import jax

jax.devices()

"""Great! We have a TPU device with 8 chips. In this notebook, we'll leverage *data parallelism* across these 8 chips, by sending 1/8 of our batch to each chip and generating the LLM outputs in parallel. TPUs shine at higher batch-sizes, where computations can easily be distributed across devices.

JAX comes pre-installed on Colab TPUs at the correct version, so there's no need to re-install it. However, we do need to updgrade Transformers to the latest version, since the Gemma model is only included in the latest library release:
"""

"""## Load the Model

The Gemma model can be loaded using the familiar [`from_pretrained`](https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained) method in Transformers. This method downloads the model weights from the Hugging Face Hub the first time it is called, and subsequently intialises the Gemma model using these weights.

We'll load the weights for the base 2B model by specifying the corresponding model-id on the Hugging Face Hub (["google/gemma-2b"](https://huggingface.co/google/gemma-2b)). You should ensure you have accepted the model terms-of-use before downloading the model. To do so, simply head to the model repository [here](https://huggingface.co/google/gemma-2b) and fill out the required fields.

We'll set the data-type (dtype) of the computation to bfloat16, which is faster than float32 while achieving similar accuracy. Finally, we'll set the flag `_do_init=False`, which improves the loading of large models in Transformers by skipping initialising a dummy set of parameters that are subsequently overriden by the pre-trained ones.
"""

from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()
hf_token = user_secrets.get_secret("HF_TOKEN")


import time
import argparse

import jax
import jax.numpy as jnp
import numpy as np
from flax import jax_utils
from flax.training.common_utils import shard

from transformers import FlaxGemmaForCausalLM, AutoTokenizer

# Add argument parsing
parser = argparse.ArgumentParser(description='Run JAX Gemma model inference')
parser.add_argument('--model_name', type=str, default="google/gemma-2b-it",
                    help='The model name or path (default: google/gemma-2b-it)')
parser.add_argument('--max_new_tokens', type=int, default=4096,
                    help='Maximum number of new tokens to generate (default: 4096)')
parser.add_argument('--batch_size', type=int, default=8,
                    help='Batch size for generation (default: 8)')
parser.add_argument('--max_input_length', type=int, default=32,
                    help='Maximum input length for tokenizer (default: 32)')
args = parser.parse_args()

model_name = args.model_name
max_new_tokens = args.max_new_tokens
batch_size = args.batch_size
max_input_length = args.max_input_length

# Check if using 7B model, and adjust batch size if needed
if "7b" in model_name.lower():
    # Use a much smaller batch size for the 7B model due to memory constraints
    batch_size = 1  # Reduce to 1 for severe memory constraints
    print(f"Using 7B model with severe memory constraints - adjusted batch size to {batch_size}")
    
    # For more memory savings with 7B model:
    max_new_tokens = min(max_new_tokens, 256)  # Generate fewer tokens
    max_input_length = min(max_input_length, 16)  # Shorter inputs
    print(f"Memory constraints: adjusted max_new_tokens to {max_new_tokens}, max_input_length to {max_input_length}")

# Even for 2B model, you might need to reduce parameters
if batch_size > 1 and "resource_exhausted" in locals():  # If you've hit the error before
    batch_size = 1
    max_new_tokens = min(max_new_tokens, 512)
    max_input_length = min(max_input_length, 24)
    print(f"Reducing parameters due to memory constraints: batch_size={batch_size}, max_new_tokens={max_new_tokens}, max_input_length={max_input_length}")

# Load the model with appropriate settings
model, params = FlaxGemmaForCausalLM.from_pretrained(
    model_name, 
    revision="flax", 
    _do_init=False, 
    dtype=jnp.bfloat16, 
    token=hf_token
)

"""If you're coming from PyTorch, the only major difference in API is how the model and parameters are handled. PyTorch is a _stateful_ framework, in which the weights are stored within the model instance. In JAX, most transformations (notably `jax.jit`) require functions that are _stateless_, meaning that they have no side effects (see [Stateful Computations](https://jax.readthedocs.io/en/latest/jax-101/07-state.html) in JAX). Since Flax models are designed to work well with JAX transformations, they too are stateless. This means that the model weights are stored **outside** of the model definition, and need to be passed as an input during inference.

We see a warning that the model parameters were loaded in bfloat16 precision - this is fine since we also want to keep the parameters in bfloat16 for inference.

The corresponding tokenizer can now be loaded using a similar API:
"""

tokenizer = AutoTokenizer.from_argument('--model_name', type=str, default="google/gemma-2b-it",
                    help='The model name or path (default: google/gemma-2b-it)')
parser.add_argument('--max_new_tokens', type=int, default=4096,
                    help='Maximum number of new tokens to generate (default: 4096)')
parser.add_argument('--batch_size', type=int, default=8,
                    help='Batch size for generation (default: 8)')
parser.add_argument('--max_input_length', type=int, default=32,
                    help='Maximum input length for tokenizer (default: 32)')
args = parser.parse_args()

model_name = args.model_name
max_new_tokens = args.max_new_tokens
batch_size = args.batch_size
max_input_length = args.max_input_length

# Check if using 7B model, and adjust batch size if needed
if "7b" in model_name.lower():
    # Use a much smaller batch size for the 7B model due to memory constraints
    batch_size = 1  # Reduce to 1 for severe memory constraints
    print(f"Using 7B model with severe memory constraints - adjusted batch size to {batch_size}")
    
    # For more memory savings with 7B model:
    max_new_tokens = min(max_new_tokens, 256)  # Generate fewer tokens
    max_input_length = min(max_input_length, 16)  # Shorter inputs
    print(f"Memory constraints: adjusted max_new_tokens to {max_new_tokens}, max_input_length to {max_input_length}")

# Even for 2B model, you might need to reduce parameters
if batch_size > 1 and "resource_exhausted" in locals():  # If you've hit the error before
    batch_size = 1
    max_new_tokens = min(max_new_tokens, 512)
    max_input_length = min(max_input_length, 24)
    print(f"Reducing parameters due to memory constraints: batch_size={batch_size}, max_new_tokens={max_new_tokens}, max_input_length={max_input_length}")

# Load the model with appropriate settings
model, params = FlaxGemmaForCausalLM.from_pretrained(
    model_name, 
    revision="flax", 
    _do_init=False, 
    dtype=jnp.bfloat16, 
    token=hf_token
)

"""If you're coming from PyTorch, the only major difference in API is how the model and parameters are handled. PyTorch is a _stateful_ framework, in which the weights are stored within the model instance. In JAX, most transformations (notably `jax.jit`) require functions that are _stateless_, meaning that they have no side effects (see [Stateful Computations](https://jax.readthedocs.io/en/latest/jax-101/07-state.html) in JAX). Since Flax models are designed to work well with JAX transformations, they too are stateless. This means that the model weights are stored **outside** of the model definition, and need to be passed as an input during inference.

We see a warning that the model parameters were loaded in bfloat16 precision - this is fine since we also want to keep the parameters in bfloat16 for inference.

The corresponding tokenizer can now be loaded using a similar API:
"""

tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)

"""## Define Inputs

Next, we'll define our text inputs. Since we have 8 TPU cores over which we want to perform data parallelism, we need our batch size to be a multiple of 8. This is to ensure that each TPU core receives the same amount of data (`bsz / 8` samples).

In this example, we'll define a single prompt and copy it 8 times, giving a total batch size of 8. The "per-device" batch size is then 1/8 of this, or 1. In practice, this is not very useful or realistic, since each of our parallel computations will be processing the same input. However, you're free to extend this to use more realistic prompts than the one given below. Just ensure that the resulting batch size is a multiple of 8.
"""

input_text = batch_size * ["Write an article about AI"]

"""We can pre-process our input text to token ids using the tokenizer. TPUs expect inputs of static shape, so we'll define our maximum prompt length to be 64, and always pad our inputs to this sequence length:"""

inputs = tokenizer(
    input_text,
    padding="max_length",
    max_length=max_input_length,
    return_attention_mask=True,
    return_tensors="np",
)

"""We now need to copy the model parameters to each TPU core. Each core will hold it's own copy of the parameters, such that it can run a model generation in parallel with the others. Copying the parameters across devices is achieved simply with the [`replicate`](https://flax.readthedocs.io/en/latest/api_reference/flax.jax_utils.html#flax.jax_utils.replicate) method from Flax."""

# Option 1: If your TPU has multiple devices but not enough memory per device
# Use model parallelism instead of data parallelism
from jax.experimental import mesh_utils
from jax.experimental.pjit import pjit
from jax.sharding import PartitionSpec

# Create a mesh for model parallelism
devices = mesh_utils.create_device_mesh((8,))  # Assuming 8 TPU cores
with jax.experimental.maps.mesh(devices, ('dp',)):
    # Partition the parameters across devices
    params = jax.experimental.pjit.pjit(
        lambda x: x,
        in_shardings=None,
        out_shardings=PartitionSpec('dp'),
    )(params)

# Option 2: If memory is still an issue, use the simplest approach - process one sample at a time
# Use a single device and lower precision
import jax.numpy as jnp
params = params.copy()  # Make a copy to avoid modifying the original
# Use even lower precision if needed
params = jax.tree_map(lambda x: x.astype(jnp.bfloat16), params)

# Choose only the first TPU device if replicate doesn't work
params = jax.device_put(params, jax.devices()[0])

"""Similarly, we need to split (or shard) our inputs across TPU cores. Given input ids of shape `(bsz, seq_len)`, we'll shard them to `(num_devices, bsz / num_devices, seq_len)`. In doing so, each device can receive a batch of shape `(bsz / num_devices, seq_len)`, in order to leverage data parallelism across TPU cores. Sharding our inputs is achieved with the Flax helper function [`shard`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.common_utils.shard):"""

inputs = shard(inputs.data)

"""## Inference

We can now define our data-parallel method for inference. The Transformers [`generate`](https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate) method provides functionality for auto-regressive generation with batching, sampling, beam-search, etc. To reap the benefits of JAX, we'll compile the generate method end-to-end, such that the operations are fused into XLA-optimised kernels and executed efficiently on our hardware accelerator.

To achieve this, we'll wrap the `generate` method with the [`jax.pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html) transformation. The `jax.pmap` transformation compiles the `generate` method with XLA, and prepares a function that can be executed in parallel across TPU devices.
"""

def generate_single_device(input_ids, attention_mask, params, max_new_tokens):
    generated_ids = model.generate(
        input_ids,
        attention_mask=attention_mask,
        params=params,
        max_new_tokens=max_new_tokens,
        do_sample=True,
    )
    return generated_ids.sequences

# Process inputs one at a time to save memory
generated_ids = []
for i in range(len(input_text)):
    single_input = {
        "input_ids": inputs["input_ids"][i:i+1],
        "attention_mask": inputs["attention_mask"][i:i+1]
    }
    result = generate_single_device(single_input["input_ids"], 
                                  single_input["attention_mask"], 
                                  params, 
                                  max_new_tokens)
    generated_ids.append(result)

# Combine results
generated_ids = jnp.concatenate(generated_ids, axis=0)

"""The generate function returns a batch of generated token ids. To convert these to generated text, we can decode them using the tokenizer:"""

generated_ids = jax.device_get(generated_ids.reshape(-1, generated_ids.shape[-1]))
pred_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

"""So how fast was generation? Let's compute the number of tokens generated per-second (tok/s):"""

def compute_tok_per_s(input_ids, generated_ids, runtime):
    total_inputs = np.prod(input_ids.shape)
    total_outputs = np.prod(generated_ids.shape)
    tokens_generated = total_outputs - total_inputs
    print(f"Tokens generated: {tokens_generated}")
    tokens_per_s = tokens_generated / runtime
    return tokens_per_s

tok_per_s = compute_tok_per_s(inputs["input_ids"], generated_ids, runtime)

"""We can then print our runtime, tokens per second, and generated text to the console:"""

print(f"Runtime with pmap: {runtime}")
print(f"Tokens per second: {tok_per_s}")
print(pred_text[0])
